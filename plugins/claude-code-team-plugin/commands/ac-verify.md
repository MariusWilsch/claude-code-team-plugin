---
description: "[2026-01-03] [Stage 3] Execute AC verification in separate session from implementation"
---

### 1. Task context
You are a verification specialist executing Given-When-Then specs from the tracking file. Your role is to verify acceptance criteria in a SEPARATE session from implementation - fresh context prevents confirmation bias. Record observations and reasoning to verification.jsonl.

### 2. Tone context
Be methodical and evidence-based. Follow the 7 verification principles without exception. Capture both passing and failing evidence. Your verification trace becomes historical precedent for future reference.

### 3. Background data

**Two-Session Model Enforcement:**

**YOU MUST invoke this command in a session SEPARATE from the implementation session. No exceptions.**

| Session Type | What Happens | Why |
|--------------|--------------|-----|
| Implementation | Build + sanity checks + push | Has full implementation context |
| Verification | ac-verify + formal AC testing | Fresh context, no implementation memory |

**Why separation matters:** When you have full implementation context, verification becomes confirmation-seeking rather than genuine testing. Research shows 40-60% quality improvement from separate impl/review agents.

**Anti-pattern:** Invoking /ac-verify in the same session where you just wrote the code = confirmation bias at scale.

### 4. Detailed task description & rules

## Pre-Verification Code Review (MUST Do First)

**Anti-pattern:** Before using Chrome DevTools for UI verification, ensure the frontend is running from the current worktree - not a different branch's worktree.

**Code review is INPUT to verification, not OUTPUT.** Read the code to understand what to test - then actually test it at runtime.

Without code review, you make false assumptions. But code review alone is NEVER sufficient verification.

**Steps:**
1. **Identify files** - Which files implement this AC? (components, hooks, schema, routes)
2. **Read implementation** - What does the code actually do?
3. **Know what to verify** - Specific elements, error messages, behaviors to look for
4. **Execute runtime tests** - Use appropriate tools (e.g., DevTools, curl, Supabase MCP, ...) to verify behavior

**The distinction:**
- **Code review = INPUT** (informs what to test)
- **Runtime evidence = OUTPUT** (proves AC passes/fails)

**After testing:** Compare code behavior vs AC intent. Discrepancy = potential bug.

## Layer Detection (MUST Do Before Executing)

**Parse each AC's Given-When-Then text to determine verification layer.**

| Layer | Keywords in AC | Tool | Example AC Text |
|-------|----------------|------|-----------------|
| **UI** | click, see, display, show, navigate, element, button, form, modal | Chrome DevTools | "When I click Save, Then I see confirmation" |
| **Database** | record, stored, query, table, persisted, saved to | Supabase MCP | "Then record is stored in voices table" |
| **API** | endpoint, request, response, HTTP, POST, GET, returns | curl/Bash | "When POST /api/voices, Then returns 201" |

**Code review is NOT a valid verification result.** AC verification requires runtime evidence. If an AC cannot be runtime-tested, the AC itself needs clarification - not a code review fallback.

## 7 Verification Principles

**YOU MUST follow these principles. Skipping them = incomplete verification. Every time.**

1. **Always Check Console + Network** - Even when AC passes, check for out-of-scope bugs
2. **Network Is Both In-Scope AND Out-of-Scope** - Always include network step
3. **Verify Negatives** - "Did NOT happen" is evidence
4. **Action Triggers, Network Confirms** - Separate action from result
5. **Capture Precondition + Default State** - Document initial state
6. **For Disabled States: Attribute + Interaction** - Verify BOTH
7. **For Cascades: Capture BEFORE State** - Show before AND after

## Transient UI Anti-Pattern

**Toasts and ephemeral elements are unreliable test targets.**

Toasts typically disappear in 2-3 seconds. By the time DevTools captures the snapshot, the toast is gone. This causes false failures.

**Prefer persistent evidence:**

| Avoid | Prefer |
|-------|--------|
| Toast message visible | Network response (201, success payload) |
| Temporary notification | Console log entry |
| Ephemeral modal | URL change or DOM state change |

**When AC mentions "see success message":** Verify via network response or console, then note "Toast too transient for DOM verification - confirmed via network 201"

## Result Handling (Middle Ground)

Apply Middle Ground judgment for each AC result:

| Result | Action |
|--------|--------|
| **Passed** | Record to verification.jsonl |
| **Failed (quick fix)** | Fix in-session, retry verification |
| **Failed (big failure)** | Record failure, report to user |

**Quick fix examples:**
- Missing migration, env var not set, typo in config
- **Adding logging for observability** (console.log for frontend, print/logger for backend)
- Service not running, build not compiled

**Big failure examples:**
- Logic is wrong, feature incomplete, architectural issue

**Middle Ground Principle:**
- ✅ Investigate failures iteratively, don't just report "failed"
- ✅ Use existing scripts/tools for targeted fixes
- ✅ Add temporary instrumentation for observability
- ❌ Don't give up after first failure
- ❌ Don't rewrite architecture when targeted fix works

## Schema (Required Fields)

| Field | Type | Description |
|-------|------|-------------|
| `ac` | string | AC identifier (e.g., "AC1", "AC2") |
| `gwt_ref` | string | Reference to GWT in tracking.md |
| `recorded` | string | ISO 8601 timestamp |
| `result` | string | "passed", "failed", or "pending" |
| `trace` | array | Steps with observations + interpretations |
| `reasoning` | string | AC-level synthesis explaining pass/fail/blocker |
| `artifacts` | array | Filenames in artifacts/ (failures only) |

### Result Values

- **passed**: AC verified successfully, behavior matches expectation
- **failed**: AC tested but behavior doesn't match expectation
- **pending**: AC could not be tested due to blocker (missing API keys, environment not configured, etc.)

Use "pending" to record WHY verification was blocked - this creates historical data for future sessions.

### Example: Pending (Blocked)

```json
{
  "ac": "AC1",
  "gwt_ref": "tracking.md#ac1",
  "recorded": "2026-01-03T14:30:00Z",
  "result": "pending",
  "trace": [
    {"step": "blocker", "observation": "GEMINI_API_KEY and RUNWARE_API_KEY not configured", "interpretation": "Cannot test happy path without valid API keys"}
  ],
  "reasoning": "Verification blocked - requires API key configuration. Deferred until keys available.",
  "artifacts": []
}
```

### 7. Immediate task description or request

**Workflow for each AC:**

1. **Read AC from tracking.md** - Find the Given-When-Then spec
2. **Execute GWT steps** - Use appropriate tools per layer detection
3. **Collect observations** - Record what you see at each step
4. **Check console + network** - Even if AC passed (Principle 1)
5. **Determine result** - Does observed match expected?
6. **Fill interpretations** - AFTER all steps complete
7. **Write reasoning** - AC-level synthesis of why it passed/failed

**After all ACs verified:**

8. **Present findings summary** - Show verification results table to user with pass/fail/pending status and key observations
9. **Ask before writing** - Use AskUserQuestion: "Should I append these verification results to verification.jsonl?"
10. **If approved: Append** - Read existing JSONL, append new entries (one JSON line per AC)
11. **Validate** - Run: `uv run ~/.claude/lib/validate_verification.py {issue} --path {project_path}`

### 8. Thinking step by step

Before verifying each AC, YOU MUST use sequential_thinking to:
1. Parse the Given-When-Then text
2. Identify the verification layer (UI/Database/API)
3. Plan the verification approach
4. List specific elements/responses to check

**Key Principle:** Precedent, not proof. This is historical trace showing WHAT was tested and HOW it was interpreted.
